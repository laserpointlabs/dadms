# LLM Service Environment Configuration
# Copy this file to .env and fill in your actual API keys

# Service Configuration
LLM_SERVICE_PORT=3006
NODE_ENV=development

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_ORGANIZATION=your_openai_org_id_here
OPENAI_BASE_URL=https://api.openai.com/v1

# Anthropic Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_BASE_URL=https://api.anthropic.com

# xAI Grok Configuration
XAI_API_KEY=your_xai_api_key_here
XAI_BASE_URL=https://api.x.ai/v1

# Ollama Configuration (Local)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=30000

# Provider Feature Flags
ENABLE_OPENAI=true
ENABLE_ANTHROPIC=true
ENABLE_XAI_GROK=true
ENABLE_OLLAMA=true

# Default Provider Settings
DEFAULT_PROVIDER=openai
FALLBACK_PROVIDER=local

# Redis Configuration (for future caching)
REDIS_URL=redis://localhost:6379
ENABLE_CACHING=false
CACHE_TTL_SECONDS=3600

# Rate Limiting
RATE_LIMIT_RPM=100
OPENAI_RATE_LIMIT_RPM=3000
ANTHROPIC_RATE_LIMIT_RPM=1000
XAI_RATE_LIMIT_RPM=1000

# Cost Management
MONTHLY_BUDGET_USD=100
COST_WARNING_THRESHOLD=0.8
RATE_LIMIT_RPH=1000
COST_LIMIT_PER_HOUR=10.00

# Logging
LOG_LEVEL=info
