openapi: 3.0.3
info:
  title: DADMS LLM Service API
  version: 1.0.0
  description: |
    OpenAPI specification for the DADMS 2.0 LLM Service. Defines endpoints for prompt generation, tool calling, model/persona/provider management, and cost estimation.
servers:
  - url: http://localhost:3002
    description: Local development server

components:
  securitySchemes:
    bearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT
  schemas:
    LLMGenerateRequest:
      type: object
      properties:
        prompt:
          type: string
        model:
          type: string
        persona:
          type: string
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1.0
          description: Controls randomness/creativity.
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1.0
          description: Nucleus sampling.
        max_tokens:
          type: integer
          default: 256
          description: Maximum output length.
        presence_penalty:
          type: number
          minimum: -2
          maximum: 2
          description: Penalizes new topics.
        frequency_penalty:
          type: number
          minimum: -2
          maximum: 2
          description: Penalizes repetition.
        stop:
          type: array
          items:
            type: string
          description: Stop sequences.
        tools:
          type: array
          items:
            type: object
          description: Tool/function definitions for tool calling.
        stream:
          type: boolean
          default: false
          description: If true, response is streamed.
    LLMModel:
      type: object
      properties:
        id:
          type: string
        name:
          type: string
        provider:
          type: string
        description:
          type: string
        capabilities:
          type: array
          items:
            type: string
        supported_parameters:
          type: array
          items:
            type: string
          description: List of supported generation parameters for this model.
    LLMProvider:
      type: object
      properties:
        id:
          type: string
        name:
          type: string
        status:
          type: string
          enum: [healthy, degraded, unavailable]
        models:
          type: array
          items:
            type: string
        capabilities:
          type: array
          items:
            type: string
        quota:
          type: object
          properties:
            used:
              type: integer
            limit:
              type: integer
        supported_parameters:
          type: array
          items:
            type: string
          description: List of supported generation parameters for this provider.
    # ... (other schemas as previously defined)

security:
  - bearerAuth: []

paths:
  /llm/health:
    get:
      summary: Service health/readiness check
      description: Returns service health status, uptime, and version.
      tags: [LLM]
      responses:
        '200':
          description: Health status
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                  uptime:
                    type: integer
                  version:
                    type: string
  # Planned endpoints for future implementation (not MVP)
  /llm/rate-limit:
    get:
      summary: (Planned) Get current rate limit/quota status
      description: Returns current usage and quota for the user/service. Planned for future implementation.
      tags: [LLM]
      responses:
        '200':
          description: Rate limit status
          content:
            application/json:
              schema:
                type: object
                properties:
                  limit:
                    type: integer
                  used:
                    type: integer
                  reset_in:
                    type: integer
  /llm/audit-log:
    get:
      summary: (Planned) Get recent LLM/tool calls for audit/compliance
      description: Returns recent LLM/tool calls for the user/service. Planned for future implementation.
      tags: [LLM]
      responses:
        '200':
          description: Audit log entries
          content:
            application/json:
              schema:
                type: array
                items:
                  type: object
                  properties:
                    timestamp:
                      type: string
                    action:
                      type: string
                    model:
                      type: string
                    user:
                      type: string
  /llm/limits:
    get:
      summary: (Planned) Get input/output size limits
      description: Returns max prompt and completion size limits. Planned for future implementation.
      tags: [LLM]
      responses:
        '200':
          description: Input/output size limits
          content:
            application/json:
              schema:
                type: object
                properties:
                  max_prompt_tokens:
                    type: integer
                  max_completion_tokens:
                    type: integer
  /llm/providers:
    get:
      summary: List available LLM providers and status
      tags: [LLM]
      responses:
        '200':
          description: List of LLM providers
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: '#/components/schemas/LLMProvider'
        default:
          description: Error
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
      security:
        - bearerAuth: []
  /llm/generate:
    post:
      summary: Generate a completion from a prompt (supports streaming)
      description: |
        Generates a completion from a prompt using the selected model/persona. If `stream: true` is set in the request body, the response will be streamed as a series of chunks (e.g., via Server-Sent Events).
      tags: [LLM]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/LLMGenerateRequest'
      responses:
        '200':
          description: Completion (streamed if requested)
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/LLMGenerateResponse'
            text/event-stream:
              schema:
                type: object
                properties:
                  content:
                    type: string
        default:
          description: Error
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
      security:
        - bearerAuth: []
  /llm/generate-async:
    post:
      summary: (Planned) Submit an async LLM generation request
      description: |
        Planned for future implementation (not MVP). Submits a generation request and returns a job/task ID for polling or callback.
      tags: [LLM]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/LLMGenerateRequest'
      responses:
        '202':
          description: Accepted, job created
          content:
            application/json:
              schema:
                type: object
                properties:
                  job_id:
                    type: string
                  status:
                    type: string
                    enum: [pending, running, completed, failed]
        default:
          description: Error
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
      security:
        - bearerAuth: []
  /llm/generate/{job_id}/status:
    get:
      summary: (Planned) Get status/result of an async LLM generation job
      description: |
        Planned for future implementation (not MVP). Returns the status and, if complete, the result of the async generation job.
      tags: [LLM]
      parameters:
        - in: path
          name: job_id
          required: true
          schema:
            type: string
      responses:
        '200':
          description: Job status and result (if complete)
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    enum: [pending, running, completed, failed]
                  result:
                    $ref: '#/components/schemas/LLMGenerateResponse'
        default:
          description: Error
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
      security:
        - bearerAuth: []
# ... (other paths as previously defined) 